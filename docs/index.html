<!doctype html>
<html lang="en">

<!-- === Header Starts === -->

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>EmotionKD</title>
    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>

<!-- === Header Ends === -->

<script>
    var lang_flag = 1;


</script>

<body>

    <!-- === Home Section Starts === -->
    <div class="section">
        <!-- === Title Starts === -->
        <div class="header">
            <!--        <div class="logo">-->
            <!--            <a href="https://decisionforce.github.io/" target="_blank">-->
            <!--                <img src="images/deciforce.png">-->
            <!--            </a>-->
            <!--        </div>-->
            <!--        <div style="padding-top: 30pt; margin: 0 50pt;" class="title" id="lang">-->
            <!--            Safe Driving via Expert Guided Policy Optimization-->
            <!--        </div>-->

            <table>
                <tr>
                    <!-- <td>
            
                    <div class="logo"
                         style="
                         width: 100pt;
                         vertical-align: text-top;
                         text-align: center;
                         ">
                        <a href="https://ai4brain.github.io/" target="_blank">
                            <img src="images/logo.png">
                        </a>
                    </div>

                    </td> -->
                    <td>
                        <div style="padding-top: 10pt;" class="title" id="lang">
                            EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals<br><br>
                        </div>

                    </td>
                    <!-- <td>
                        <div class="logo" style="
                         width: 100pt;
                         padding-top: 10pt;
                         vertical-align: center;
                         text-align: center;
                         ">
                            <a href="https://github.com/decisionforce/metadrive" target="_blank">
                                <img style=" width: 120pt;" src="images/logo.png">
                            </a>
                        </div>

                    </td> -->
                </tr>
            </table>


        </div>
        <!-- === Title Ends === -->
        <div class="author">
            <p style="text-align:center">ACM Multimedia 2023<br><br></p>
            <a href="">Yucheng Liu</a><sup>1</sup>,&nbsp;
            <a href="https://ziyujia.github.io/" target="_blank">Ziyu Jia</a><sup>1</sup>,&nbsp;&nbsp;
            <a href="https://hychaowang.github.io/" target="_blank">Haichao Wang</a><sup>2</sup>,&nbsp;
        </div>

        <div class="institution" style="font-size: 11pt;">
            <div>
                <sup>1</sup>Institute of Automation, Chinese Academy of Sciences<br>
                <sup>2</sup>Tsinghua-Berkeley Shenzhen Institute<br>
                <br><br>
            </div>
        </div>
        <table border="0" align="center">
            <tr>
                <td align="center" style="padding: 0pt 0 15pt 0">
                    <a class="bar" href=" "><b>Webpage</b></a> |
                    <a class="bar" href="https://github.com/YuchengLiu-Alex/EmotionKD"><b>Code</b></a> |
                    <a class="bar" href="#video"><b>Video</b></a> |
                    <a class="bar" href="https://dl.acm.org/doi/10.1145/3581783.3612277"><b>Paper</b></a> |
                    <a class="bar" href="./Poster/poster.pdf"><b>Poster</b></a>
                </td>
            </tr>
        </table>

    </div>
    <!-- === Home Section Ends === -->

    <!-- === Overview Section Starts === -->
    <div class="section">
        <div class="title" id="lang">EmotionKD</div>
        <div class="body">

            <div class="teaser">
                <img src="images/Pipeline.png">
                <div class="text">
                    <br>
                    Fig. 1 Structure of proposed EmotionKD. The knowledge in the multi-modal model is transferred to the unimodal model through knowledge distillation. The IMF feature is obtained from the interactivity-based modal fusion module.
                </div>
            </div>

            <div class="text">
                <p>
                    We design knowledge distillation framework to improve the performance of unimodal student models through knowledge distillation. Specifically, the EmotionKD framework includes the following modules: <br>
                    <ul>
                        <li> A multi-modal teacher model called EmotionNet-Teacher;</li> 

                        <li>A unimodal student model called EmotionNet-Student;</li>

                        <li>An adaptive feedback knowledge distillation.</li>

                   </ul>
                </p>
            </div>

            <div class="teaser">
                <img src="images/Emotion-Teacher.png">
                <div class="text">
                    <br>
                    Fig. 2 Structure of proposed Emotion-Teacher network. 
                </div>
            </div>

            <div class="text">
                <p>
                    In the Emotion-Teacher model, the heterogeneity fram each modality is extracted by transformer encoder. As for the interactivity, it is extracted from the heterogeneity feature by the IMF module. Finally, we employ the IMF feature to obtain the higher performance.
                </p>
            </div>

            <div class="centered-content">
                <img src="images/Feedback.png"; width="500px">
                <div class="text">
                    <br>
                    Fig. 3 The adaptive feadback knowledge distillation method. 
                </div>
            </div>

            <div class="text">
                <p>
                    Adaptive feedback knowledge distillation method is proposed in this paper so that the teacher model is able to adjust the ouput feature during the knowledge distillation. Hence, student model will have better better performacne.<br>
                </p>
            </div>

            <div class="text">
                <p>
                    Our contributions are as follows: <br>
                    <ul>
                         <li>We propose a novel multi-modal EmotionNet-Teacher with an <b>Interactivity-based Modal Fusion (IMF) module</b>;<br></li> 

                         <li>
                            We design an <b>adaptive feedback mechanism</b> for cross-modal knowledge distillation;<br>
                         </li>

                         <li>
                            To the best of our knowledge, this is the <b>first application</b> of cross-modal knowledge distillation in the field of physiological signal-based emotion recognition to transfer fused EEG and GSR features to the unimodal GSR model.<br>
                         </li>

                    </ul>
                </p>
            </div>


        </div>
    </div>

<!-- === Result Section Starts === -->
    <div class="section">
        <div class="title" id="lang">Experiments</div>
        <div class="body">

            <div>

                <div class="text">
                    We firstly compare our EmotionNet-Student with other methods. The results show that EmotionNet-Student has a SOTA performance compared to other unimodal or knowledge transfer-based baseline methods on the emotion recognition task. It proves that our enhanced unimodal model is better than other unimodal methods.
                    <br>
                </div>
                <img src="images/results/Results_UniModal.png"; text-align: center; width="950px">

                <div class="text">
                    Then, we compare our EmotionNet-Teachaer with other baselines. The results show that the proposed EmotionNet-Teacher has a significantly better performance compared to other multimodal baseline methods on the emotion recognition task. It also proves that our proposed multimodal method is effective.
                    <br>
                </div>
                <img src="images/results/Results_MultiModal.png"; text-align: center; width="950px">

            </div>

        </div>
    </div>
    </div>

<!-- === Result Section Ends === -->

<div class="section">
    <div class="title" id="video">Talk</div>
    <video width="900" height="506" controls>
        <source src="videos/mmfp2527-video.mp4" type="video/mp4">
      </video>
</div>

    <!-- === Reference Section Starts === -->
    <div class="section">
        <div class="bibtex">
            <div class="text">Reference</div>
        </div>
        This paper has not been online yet.
        <pre>

    </pre>
        <!-- Adjust the frame size based on the demo (Every project differs). -->
    </div>

</body>

</html>